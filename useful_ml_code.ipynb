{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = event[\"queryStringParameters\"][\"content\"]\n",
    "\n",
    "res_body = {}\n",
    "res_body[\"content\"] = content\n",
    "\n",
    "http_res = {}\n",
    "http_res[\"statusCode\"] = 200\n",
    "http_res[\"headers\"] = {}\n",
    "http_res[\"headers\"][\"Content-Type\"] = \"application/json\"\n",
    "http_res[\"body\"] = json.dumps(res_body)\n",
    "return http_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap for correlations of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(df: pd.core.frame.DataFrame, title_name: str='Train correlation') -> None:\n",
    "\n",
    "    corr = df.corr()\n",
    "    fig, axes = plt.subplots(figsize=(12, 8))\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    sns.heatmap(corr, mask=mask, linewidths=.5, cmap='RdBu_r', annot=True,annot_kws={\"size\": 8})\n",
    "    plt.title(title_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_correlation_heatmap(df, 'Train Dataset Correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "def summary(df):\n",
    "    print(f'data shape: {df.shape}')\n",
    "    summ = pd.DataFrame(df.dtypes, columns=['data type'])\n",
    "    summ['#missing'] = df.isnull().sum().values \n",
    "    summ['%missing'] = df.isnull().sum().values / len(df) * 100\n",
    "    summ['#unique'] = df.nunique().values\n",
    "    desc = pd.DataFrame(df.describe(include='all').transpose())\n",
    "    summ['min'] = desc['min'].values\n",
    "    summ['max'] = desc['max'].values\n",
    "    summ['average'] = desc['mean'].values\n",
    "    summ['standard_deviation'] = desc['std'].values\n",
    "    summ['first value'] = df.loc[0].values\n",
    "    summ['second value'] = df.loc[1].values\n",
    "    summ['third value'] = df.loc[2].values\n",
    "    \n",
    "    return summ\n",
    "\n",
    "summary(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "class DataPrep:\n",
    "    def __init__(self, train, test, idcol, target_col):\n",
    "        self.train_ids = train #just the id column\n",
    "        self.train_data = train #everything from train\n",
    "        self.train = train.drop(columns=[idcol, target_col]) #just the data\n",
    "        self.train_target = train[target_col] #Just the target\n",
    "        self.test_ids = test.id #Test IDs\n",
    "        self.test = test.drop(columns=[idcol]) #Just the test data\n",
    "        self.normalize()\n",
    "        self.SKfold(10)\n",
    "\n",
    "    def SKfold(self, splits):\n",
    "        self.skf = StratifiedKFold(n_splits=splits, shuffle=True)\n",
    "        \n",
    "    def drop_cols(self, columns):\n",
    "        self.train = self.train.drop(columns=columns)\n",
    "        self.test = self.test.drop(columns=columns)\n",
    "\n",
    "    def normalize(self):\n",
    "        self.ss = StandardScaler()\n",
    "        new_train = self.ss.fit_transform(self.train)\n",
    "        self.scaled_train = pd.DataFrame(new_train, index=self.train.index, columns=self.train.columns)\n",
    "        new_test = self.ss.transform(self.test)\n",
    "        self.scaled_test = pd.DataFrame(new_test, index=self.test.index, columns=self.test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cross_val_score and cross_val predict from sklearn.model_selection to skip the having to do the kfolds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Optuna for Optimization?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
